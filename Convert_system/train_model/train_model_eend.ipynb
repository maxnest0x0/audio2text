{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12121634,"sourceType":"datasetVersion","datasetId":7632631},{"sourceId":12135261,"sourceType":"datasetVersion","datasetId":7642197}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pydub import AudioSegment\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport librosa\nimport seaborn as sns\n ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TARGET_LENGTH = 30.0   \nSAMPLE_RATE = 16000\nAUDIO_DIR = \"/kaggle/input/diarization/Dataset/data/audio\"\nRTTM_DIR = \"/kaggle/input/diarization/Dataset/data/markups\"\nOUTPUT_AUDIO_DIR = \"/kaggle/working/new_audio\"\nOUTPUT_RTTM_DIR = \"/kaggle/working/new_rttm\"\n\nos.makedirs(OUTPUT_AUDIO_DIR, exist_ok=True)\nos.makedirs(OUTPUT_RTTM_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **EDA** ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef parse_rttm(file_path):\n    columns = ['type', 'file_id', 'channel', 'start', 'duration', 'ortho', 'stype', 'name', 'conf', 'slat']\n    data = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            if line.strip():\n                parts = line.strip().split()\n                data.append(parts[:len(columns)])\n    return pd.DataFrame(data, columns=columns)\n\nrttm_files = [f'/kaggle/input/diarization/Dataset/data/markups/{i}.rttm' for i in range(1, 449)]   \nall_rttm = pd.concat([parse_rttm(f) for f in rttm_files])\nall_rttm['start'] = all_rttm['start'].astype(float)\nall_rttm['duration'] = all_rttm['duration'].astype(float)\nall_rttm['end'] = all_rttm['start'] + all_rttm['duration']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport matplotlib.pyplot as plt\n\naudio_files = [f'/kaggle/input/diarization/Dataset/data/audio/{i}.wav' for i in range(1, 449)]\ndurations = []\nfor audio_file in audio_files:\n    dur = librosa.get_duration(path=audio_file)\n    durations.append(dur)\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.hist(durations, bins=20)\nplt.title(\"Распределение длительности аудиофайлов\")\nplt.xlabel(\"Длительность (сек)\")\nplt.ylabel(\"Количество файлов\")\n \n\nplt.subplot(1, 2, 2)\nspeakers_per_file = all_rttm.groupby('file_id')['name'].nunique()\nplt.hist(speakers_per_file, bins=10)\nplt.title(\"Количество спикеров на файл\")\nplt.xlabel(\"Число спикеров\")\nplt.ylabel(\"Количество файлов\")\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.hist(all_rttm['duration'], bins=50)\nplt.title(\"Распределение длительности реплик\")\nplt.xlabel(\"Длительность (сек)\")\nplt.ylabel(\"Количество сегментов\")\n\nplt.subplot(1, 2, 2)\nspeaker_duration = all_rttm.groupby('name')['duration'].sum().sort_values(ascending=False)\nspeaker_duration.plot(kind='bar')\nplt.title(\"Общее время речи спикеров\")\nplt.xlabel(\"Спикер\")\nplt.ylabel(\"Суммарная длительность (сек)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import combinations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_overlaps(df):\n    overlaps = 0\n    speakers = df['name'].unique()\n    for spk1, spk2 in combinations(speakers, 2):\n        segments1 = df[df['name'] == spk1][['start', 'end']].values\n        segments2 = df[df['name'] == spk2][['start', 'end']].values\n         \n        for s1 in segments1:\n            for s2 in segments2:\n                if max(s1[0], s2[0]) < min(s1[1], s2[1]):\n                    overlaps += 1\n    return overlaps\n\noverlaps_per_file = all_rttm.groupby('file_id').apply(check_overlaps)\nprint(f\"Среднее число наложений на файл: {overlaps_per_file.mean()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef analyze_speaker_balance(rttm_dir):\n    all_rttm = []\n    for rttm_file in os.listdir(rttm_dir):\n        if rttm_file.endswith('.rttm'):\n            df = pd.read_csv(os.path.join(rttm_dir, rttm_file), \n                           sep=' ', header=None,\n                           names=['type', 'file_id', 'channel', 'start', \n                                  'duration', 'ortho', 'stype', 'name', \n                                  'conf', 'slat'])\n            all_rttm.append(df)\n    \n    all_rttm = pd.concat(all_rttm)\n    \n    # Анализ по времени речи\n    speaker_stats = all_rttm.groupby('name').agg(\n        total_duration=('duration', 'sum'),\n        segment_count=('duration', 'count'),\n        avg_duration=('duration', 'mean')\n    ).sort_values('total_duration', ascending=False)\n    \n    # Визуализация\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 2, 1)\n    speaker_stats['total_duration'].head(20).plot(kind='bar')\n    plt.title('Топ-20 спикеров по времени речи')\n    plt.ylabel('Секунды')\n    \n    plt.subplot(1, 2, 2)\n    speaker_stats['segment_count'].head(20).plot(kind='bar', color='orange')\n    plt.title('Топ-20 спикеров по количеству реплик')\n    plt.ylabel('Количество')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return speaker_stats\n\nspeaker_stats = analyze_speaker_balance(RTTM_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"speaker_stats","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"speakers_per_file = all_rttm.groupby('file_id')['name'].nunique()\n\nplt.figure(figsize=(12, 6))\nsns.histplot(speakers_per_file, bins=10)\nplt.title(\"Количество спикеров на файл\")\nplt.xlabel(\"Число спикеров\")\nplt.ylabel(\"Количество файлов\")\nplt.show()\n\nprint(f\"Среднее количество спикеров на файл: {speakers_per_file.mean():.2f}\")\nprint(f\"Максимальное количество спикеров в одном файле: {speakers_per_file.max()}\")\nprint(f\"Минимальное количество спикеров в одном файле: {speakers_per_file.min()}\")\nprint(f\"Наиболее частое количество спикеров: {speakers_per_file.mode().values[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio_files = [f'{AUDIO_DIR}/{i}.wav' for i in range(1, 449)]\ndurations = []\nfor audio_file in audio_files:\n    dur = librosa.get_duration(path=audio_file)\n    durations.append(dur)\n\n# Создание DataFrame с длительностями аудио\naudio_df = pd.DataFrame({\n    'file_id': range(1, 449),\n    'duration': durations\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.histplot(audio_df['duration'], bins=30, kde=True)\nplt.title(\"Распределение длительности аудиофайлов\")\nplt.xlabel(\"Длительность (сек)\")\nplt.ylabel(\"Количество файлов\")\nplt.show()\n\nprint(f\"Средняя длительность аудиофайла: {audio_df['duration'].mean():.2f} сек\")\nprint(f\"Медианная длительность аудиофайла: {audio_df['duration'].median():.2f} сек\")\nprint(f\"Минимальная длительность: {audio_df['duration'].min():.2f} сек\")\nprint(f\"Максимальная длительность: {audio_df['duration'].max():.2f} сек\")\nprint(f\"Общая длительность всех аудиофайлов: {audio_df['duration'].sum()/3600:.2f} часов\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.histplot(all_rttm['duration'], bins=50)\nplt.title(\"Распределение длительности реплик\")\nplt.xlabel(\"Длительность (сек)\")\nplt.ylabel(\"Количество сегментов\")\nplt.show()\n\nprint(f\"Средняя длительность реплики: {all_rttm['duration'].mean():.2f} сек\")\nprint(f\"Медианная длительность реплики: {all_rttm['duration'].median():.2f} сек\")\nprint(f\"Минимальная длительность реплики: {all_rttm['duration'].min():.2f} сек\")\nprint(f\"Максимальная длительность реплики: {all_rttm['duration'].max():.2f} сек\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"speaker_duration = all_rttm.groupby('name')['duration'].sum().sort_values(ascending=False)\n\nplt.figure(figsize=(12, 6))\nspeaker_duration.plot(kind='bar')\nplt.title(\"Общее время речи спикеров\")\nplt.xlabel(\"Спикер\")\nplt.ylabel(\"Суммарная длительность (сек)\")\nplt.xticks(rotation=45)\nplt.show()\n\nprint(f\"Общее количество уникальных спикеров: {len(speaker_duration)}\")\nprint(f\"Спикер с наибольшим временем речи: {speaker_duration.idxmax()} ({speaker_duration.max():.2f} сек)\")\nprint(f\"Спикер с наименьшим временем речи: {speaker_duration.idxmin()} ({speaker_duration.min():.2f} сек)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"segments_per_file = all_rttm.groupby('file_id').size()\n\nplt.figure(figsize=(12, 6))\nsns.histplot(segments_per_file, bins=30)\nplt.title(\"Количество реплик на файл\")\nplt.xlabel(\"Количество реплик\")\nplt.ylabel(\"Количество файлов\")\nplt.show()\n\nprint(f\"Среднее количество реплик на файл: {segments_per_file.mean():.2f}\")\nprint(f\"Максимальное количество реплик в одном файле: {segments_per_file.max()}\")\nprint(f\"Минимальное количество реплик в одном файле: {segments_per_file.min()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Выберем несколько случайных файлов для визуализации\nsample_files = all_rttm['file_id'].sample(5).unique()\n\nplt.figure(figsize=(15, 10))\nfor i, file_id in enumerate(sample_files, 1):\n    file_data = all_rttm[all_rttm['file_id'] == file_id]\n    plt.subplot(5, 1, i)\n    for _, row in file_data.iterrows():\n        plt.plot([row['start'], row['end']], [row['name'], row['name']], marker='o')\n    plt.title(f\"Распределение реплик по времени (файл {file_id})\")\n    plt.xlabel(\"Время (сек)\")\n    plt.ylabel(\"Спикер\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Создаем сводную таблицу с характеристиками каждого файла\nfile_stats = pd.DataFrame({\n    'duration': audio_df['duration'],\n    'num_speakers': speakers_per_file.reset_index()['name'],\n    'num_segments': segments_per_file.reset_index()[0]\n}).reset_index()\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=file_stats, x='duration', y='num_speakers')\nplt.title(\"Длительность аудио vs Количество спикеров\")\n\nplt.subplot(1, 2, 2)\nsns.scatterplot(data=file_stats, x='duration', y='num_segments')\nplt.title(\"Длительность аудио vs Количество реплик\")\nplt.tight_layout()\nplt.show()\n\n# Вычисление коэффициентов корреляции\ncorr_speakers = file_stats['duration'].corr(file_stats['num_speakers'])\ncorr_segments = file_stats['duration'].corr(file_stats['num_segments'])\n\nprint(f\"Корреляция между длительностью и количеством спикеров: {corr_speakers:.2f}\")\nprint(f\"Корреляция между длительностью и количеством реплик: {corr_segments:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Функция для вычисления пауз между репликами в файле\ndef calculate_pauses(file_data):\n    file_data = file_data.sort_values('start')\n    pauses = []\n    for i in range(1, len(file_data)):\n        prev_end = file_data.iloc[i-1]['end']\n        curr_start = file_data.iloc[i]['start']\n        pause = curr_start - prev_end\n        if pause > 0:  # Исключаем перекрывающиеся реплики\n            pauses.append(pause)\n    return pauses\n\n# Собираем паузы для всех файлов\nall_pauses = []\nfor file_id in all_rttm['file_id'].unique():\n    file_data = all_rttm[all_rttm['file_id'] == file_id]\n    pauses = calculate_pauses(file_data)\n    all_pauses.extend(pauses)\n\n# Визуализация распределения пауз\nplt.figure(figsize=(12, 6))\nsns.histplot(all_pauses, bins=50)\nplt.title(\"Распределение длительности пауз между репликами\")\nplt.xlabel(\"Длительность паузы (сек)\")\nplt.ylabel(\"Количество пауз\")\nplt.show()\n\nprint(f\"Средняя длительность паузы: {pd.Series(all_pauses).mean():.2f} сек\")\nprint(f\"Медианная длительность паузы: {pd.Series(all_pauses).median():.2f} сек\")\nprint(f\"Минимальная пауза: {pd.Series(all_pauses).min():.2f} сек\")\nprint(f\"Максимальная пауза: {pd.Series(all_pauses).max():.2f} сек\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Среднее количество спикеров на файл: {speakers_per_file.mean():.2f}\")\nprint(f\"Максимальное количество спикеров в одном файле: {speakers_per_file.max()}\")\nprint(f\"Минимальное количество спикеров в одном файле: {speakers_per_file.min()}\")\nprint(f\"Наиболее частое количество спикеров: {speakers_per_file.mode().values[0]}\")\n\nprint(f\"Средняя длительность аудиофайла: {audio_df['duration'].mean():.2f} сек\")\nprint(f\"Медианная длительность аудиофайла: {audio_df['duration'].median():.2f} сек\")\nprint(f\"Минимальная длительность: {audio_df['duration'].min():.2f} сек\")\nprint(f\"Максимальная длительность: {audio_df['duration'].max():.2f} сек\")\nprint(f\"Общая длительность всех аудиофайлов: {audio_df['duration'].sum()/3600:.2f} часов\")\n\nprint(f\"Средняя длительность реплики: {all_rttm['duration'].mean():.2f} сек\")\nprint(f\"Медианная длительность реплики: {all_rttm['duration'].median():.2f} сек\")\nprint(f\"Минимальная длительность реплики: {all_rttm['duration'].min():.2f} сек\")\nprint(f\"Максимальная длительность реплики: {all_rttm['duration'].max():.2f} сек\")\n\nprint(f\"Общее количество уникальных спикеров: {len(speaker_duration)}\")\nprint(f\"Спикер с наибольшим временем речи: {speaker_duration.idxmax()} ({speaker_duration.max():.2f} сек)\")\nprint(f\"Спикер с наименьшим временем речи: {speaker_duration.idxmin()} ({speaker_duration.min():.2f} сек)\")\n\nprint(f\"Среднее количество реплик на файл: {segments_per_file.mean():.2f}\")\nprint(f\"Максимальное количество реплик в одном файле: {segments_per_file.max()}\")\nprint(f\"Минимальное количество реплик в одном файле: {segments_per_file.min()}\")\n\nprint(f\"Корреляция между длительностью и количеством спикеров: {corr_speakers:.2f}\")\nprint(f\"Корреляция между длительностью и количеством реплик: {corr_segments:.2f}\")\n\n\nprint(f\"Средняя длительность паузы: {pd.Series(all_pauses).mean():.2f} сек\")\nprint(f\"Медианная длительность паузы: {pd.Series(all_pauses).median():.2f} сек\")\nprint(f\"Минимальная пауза: {pd.Series(all_pauses).min():.2f} сек\")\nprint(f\"Максимальная пауза: {pd.Series(all_pauses).max():.2f} сек\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Preprocessing** ","metadata":{}},{"cell_type":"code","source":"import glob\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport librosa\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_rttm(rttm_path):\n    columns = [\"type\", \"file_id\", \"channel\", \"start\", \"duration\", \n               \"ortho\", \"stype\", \"speaker_id\", \"conf\", \"slat\"]\n    data = []\n    with open(rttm_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) < 8:\n                continue\n            data.append({\n                \"file_id\": parts[1],\n                \"start\": float(parts[3]),\n                \"duration\": float(parts[4]),\n                \"speaker_id\": parts[7]\n            })\n    return pd.DataFrame(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_audio_duration(audio_path):\n    duration = librosa.get_duration(filename=audio_path)\n    return duration\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_dataset(audio_dir, rttm_dir):\n    audio_files = list(Path(audio_dir).glob(\"*.wav\"))  # или .mp3\n    dataset = []\n    \n    for audio_path in tqdm(audio_files):\n        file_id = audio_path.stem  # file1.wav -> file1\n        duration = get_audio_duration(audio_path)\n        \n        # Парсим соответствующий .rttm\n        rttm_path = Path(rttm_dir) / f\"{file_id}.rttm\"\n        if not rttm_path.exists():\n            continue\n        \n        df_rttm = parse_rttm(rttm_path)\n        num_speakers = df_rttm[\"speaker_id\"].nunique()\n        num_segments = len(df_rttm)\n        \n        dataset.append({\n            \"file_id\": file_id,\n            \"duration\": duration,\n            \"num_speakers\": num_speakers,\n            \"num_segments\": num_segments,\n            \"speakers\": list(df_rttm[\"speaker_id\"].unique()),\n            \"audio_path\": str(audio_path),\n            \"rttm_path\": str(rttm_path)\n        })\n    \n    return pd.DataFrame(dataset)\n\n\naudio_dir = \"/kaggle/input/diarization/Dataset/data/audio\"\nrttm_dir = \"/kaggle/input/diarization/Dataset/data/markups\"\n\ndf = build_dataset(audio_dir, rttm_dir)\ndf.to_csv(\"audio_dataset.csv\", sep =';', index=False)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf = pd.read_csv(\"/kaggle/working/audio_dataset.csv\", sep=';')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_balanced = df[(df['num_speakers'] >= 2) & (df['num_speakers'] <= 7)].copy()\n\n# Разделение длинных файлов (пример для файлов >600 сек)\nmax_duration = 600\ndf_long = df_balanced[df_balanced['duration'] > max_duration]\ndf_balanced = df_balanced[df_balanced['duration'] <= max_duration]\n\n# Для каждого длинного файла создаем сегменты (примерно по 300 сек)\nsegments = []\nfor _, row in df_long.iterrows():\n    num_segments = int(row['duration'] // max_duration) + 1\n    for i in range(num_segments):\n        segment = row.copy()\n        segment['duration'] = max_duration if i < num_segments - 1 else row['duration'] % max_duration\n        segments.append(segment)\n\n# Добавляем сегменты обратно в датасет\ndf_balanced = pd.concat([df_balanced, pd.DataFrame(segments)], ignore_index=True)\n\n# Балансировка по количеству спикеров (например, оставляем 100 файлов для каждой группы)\nmin_samples = 100\ndf_balanced = df_balanced.groupby('num_speakers').apply(lambda x: x.sample(min(min_samples, len(x))).reset_index(drop=True))\n\n# Сохранение сбалансированного датасета\ndf_balanced.to_csv('balanced_audio_dataset.csv', sep=';', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. Распределение количества спикеров\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df_balanced, x='num_speakers')\nplt.title('Распределение количества спикеров после балансировки')\nplt.xlabel('Количество спикеров')\nplt.ylabel('Количество файлов')\nplt.show()\n\n# 2. Распределение длительности файлов\nplt.figure(figsize=(10, 6))\nsns.histplot(df_balanced['duration'], bins=30)\nplt.title('Распределение длительности аудиофайлов')\nplt.xlabel('Длительность (сек)')\nplt.ylabel('Количество файлов')\nplt.axvline(df_balanced['duration'].mean(), color='r', linestyle='--', label=f'Среднее: {df_balanced[\"duration\"].mean():.1f} сек')\nplt.legend()\nplt.show()\n\n# 3. Соотношение количества спикеров и длительности\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=df_balanced, x='num_speakers', y='duration')\nplt.title('Распределение длительности по количеству спикеров')\nplt.xlabel('Количество спикеров')\nplt.ylabel('Длительность (сек)')\nplt.show()\n\n# 4. Распределение количества реплик\nplt.figure(figsize=(10, 6))\nsns.histplot(df_balanced['num_segments'], bins=30)\nplt.title('Распределение количества реплик в файлах')\nplt.xlabel('Количество реплик')\nplt.ylabel('Количество файлов')\nplt.axvline(df_balanced['num_segments'].mean(), color='r', linestyle='--', label=f'Среднее: {df_balanced[\"num_segments\"].mean():.1f}')\nplt.legend()\nplt.show()\n\n# 5. Соотношение количества спикеров и реплик\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df_balanced, x='num_speakers', y='num_segments', alpha=0.6)\nplt.title('Соотношение количества спикеров и реплик')\nplt.xlabel('Количество спикеров')\nplt.ylabel('Количество реплик')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Общая длительность всех аудиофайлов: {df_balanced['duration'].sum()/3600:.2f} часов\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dataset**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport soundfile as sf\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EENDDataset(Dataset):\n    def __init__(self, csv_file, frame_size=512, frame_shift=256, chunk_size=2000, hop_size=2000, max_speakers=7, transform=None):\n        self.data = pd.read_csv(csv_file, sep=';')\n        self.frame_size = frame_size\n        self.frame_shift = frame_shift\n        self.chunk_size = chunk_size\n        self.hop_size = hop_size if hop_size is not None else chunk_size // 2\n        self.max_speakers = max_speakers\n        self.transform = transform\n        self.label_encoder = LabelEncoder()\n        \n        all_speakers = []\n        for speakers in self.data['speakers']:\n            all_speakers.extend(eval(speakers))\n        self.label_encoder.fit(list(set(all_speakers)) + ['overlap', 'silence'])\n        \n        self.chunk_indices = self._generate_chunk_indices()\n    \n    def _generate_chunk_indices(self):\n        chunk_indices = []\n        for idx in range(len(self.data)):\n            row = self.data.iloc[idx]\n            audio_path = row['audio_path']\n            \n            audio, sr = sf.read(audio_path)\n            if len(audio.shape) > 1:\n                audio = audio.mean(axis=1)\n                \n            num_frames = (len(audio) - self.frame_size) // self.frame_shift + 1\n            num_chunks = (num_frames - self.chunk_size) // self.hop_size + 1\n            \n            for i in range(num_chunks):\n                start_frame = i * self.hop_size\n                end_frame = start_frame + self.chunk_size\n                chunk_indices.append((idx, start_frame, end_frame))\n        print(f\"Создано {len(chunk_indices)} чанков \")        \n        return chunk_indices\n    \n    def __len__(self):\n        return len(self.chunk_indices)\n    \n    def __getitem__(self, chunk_idx):\n        file_idx, start_frame, end_frame = self.chunk_indices[chunk_idx]\n        row = self.data.iloc[file_idx]\n        audio_path = row['audio_path']\n        rttm_path = row['rttm_path']\n        num_speakers = row['num_speakers']\n\n        audio, sr = sf.read(audio_path)\n        if len(audio.shape) > 1:\n            audio = audio.mean(axis=1)\n\n     \n        mel_spec = librosa.feature.melspectrogram(\n            y=audio,\n            sr=sr,\n            n_fft=self.frame_size,\n            hop_length=self.frame_shift,\n            n_mels=80   \n        )\n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)   \n        mel_spec = torch.FloatTensor(mel_spec.T)  \n\n     \n        start_frame = max(0, min(start_frame, mel_spec.shape[0] - self.chunk_size))\n        end_frame = start_frame + self.chunk_size\n        mel_spec = mel_spec[start_frame:end_frame]\n\n         \n        if mel_spec.shape[0] < self.chunk_size:\n            pad_len = self.chunk_size - mel_spec.shape[0]\n            mel_spec = torch.cat([mel_spec, torch.zeros(pad_len, 80)], dim=0)\n\n        labels = self.parse_rttm(rttm_path, len(audio), sr)\n        labels = labels[start_frame:end_frame]\n\n        sample = {\n            'features': mel_spec,   \n            'labels': torch.FloatTensor(labels),\n            'num_speakers': num_speakers\n        }\n        return sample\n    \n    def parse_rttm(self, rttm_path, audio_length, sr):\n        num_frames = (audio_length - self.frame_size) // self.frame_shift + 1\n        frame_duration = self.frame_size / sr\n        \n        labels = np.zeros((num_frames, self.max_speakers), dtype=np.float32)\n        \n        try:\n            with open(rttm_path, 'r') as f:\n                lines = f.readlines()\n        except FileNotFoundError:\n            return labels\n            \n        speaker_mapping = {}\n        \n        for line in lines:\n            parts = line.strip().split()\n            if len(parts) < 9:\n                continue\n                \n            speaker_id = parts[7]\n            start_time = float(parts[3])\n            duration = float(parts[4])\n            end_time = start_time + duration\n            \n            start_frame = int(start_time / frame_duration)\n            end_frame = int(end_time / frame_duration)\n            \n            start_frame = max(0, min(start_frame, num_frames - 1))\n            end_frame = max(0, min(end_frame, num_frames - 1))\n            \n            if speaker_id not in speaker_mapping:\n                if len(speaker_mapping) >= self.max_speakers:\n                    continue\n                speaker_mapping[speaker_id] = len(speaker_mapping)\n            \n            spk_idx = speaker_mapping[speaker_id]\n            labels[start_frame:end_frame+1, spk_idx] = 1\n            \n        overlap = np.sum(labels, axis=1) > 1\n        if np.any(overlap) and len(speaker_mapping) < self.max_speakers:\n            overlap_channel = len(speaker_mapping)\n            if overlap_channel < self.max_speakers:\n                labels[overlap, overlap_channel] = 1\n                for spk_idx in range(len(speaker_mapping)):\n                    labels[overlap, spk_idx] = 0\n                    \n        return labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model=512):\n        super().__init__()\n        self.d_model = d_model\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, seq_len, _ = x.shape\n        pe = torch.zeros(seq_len, self.d_model, device=x.device)\n\n        position = torch.arange(seq_len, device=x.device).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, self.d_model, 2, device=x.device) * (-math.log(10000.0) / self.d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        return x + pe.unsqueeze(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    sample_rate = 16000\n    feature_dim = 80  # MFCC features\n    hidden_size = 256\n    num_layers = 4\n    max_speakers = 7  # Максимальное количество спикеров в датасете\n    dropout = 0.1\n    learning_rate = 0.0001\n    batch_size = 8\n    epochs = 50\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_save_path = \"best_eend_model.pth\"\n    metrics_save_path = \"training_metrics.json\"\n    max_audio_length = 600  # Максимальная длина аудио в секундах (10 минут)\n    segment_length = 30  # Длина сегмента в секундах для батчинга\n    overlap = 5  # Перекрытие сегментов в секундах\n\n# Модель EEND\nclass EENDModel(nn.Module):\n    def __init__(self ):\n        super(EENDModel, self).__init__()\n        config = Config()\n        \n        # Encoder для обработки аудио признаков\n        self.encoder = nn.LSTM(\n            input_size=config.feature_dim,\n            hidden_size=config.hidden_size,\n            num_layers=config.num_layers,\n            dropout=config.dropout if config.num_layers > 1 else 0,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        # Speaker prediction layers\n        self.speaker_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(2 * config.hidden_size, config.hidden_size),\n                nn.ReLU(),\n                nn.Linear(config.hidden_size, 1),\n                nn.Sigmoid()\n            ) for _ in range(config.max_speakers)\n        ])\n        \n    def forward(self, x):\n        # x: (batch_size, seq_len, feature_dim)\n        outputs, _ = self.encoder(x)  # (batch_size, seq_len, 2*hidden_size)\n        \n        # Для каждого потенциального спикера предсказываем вероятность его присутствия\n        speaker_probs = [layer(outputs) for layer in self.speaker_layers]\n        speaker_probs = torch.stack(speaker_probs, dim=-1)  # (batch_size, seq_len, max_speakers)\n        \n        return speaker_probs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Loss**","metadata":{}},{"cell_type":"code","source":"def permutation_invariant_loss(outputs, targets, num_speakers):\n    \"\"\"Улучшенная версия PIT loss с автоматической корректировкой размерностей\"\"\"\n    # Проверка размерностей\n    if outputs.dim() != targets.dim():\n        if outputs.dim() == 4 and targets.dim() == 3:\n            outputs = outputs.squeeze(1)  # Удаляем dimension каналов [B,C,T,S] -> [B,T,S]\n        else:\n            raise ValueError(f\"Dimension mismatch: outputs {outputs.shape}, targets {targets.shape}\")\n    \n    batch_size, seq_len, max_speakers = outputs.shape\n    total_loss = 0.0\n    \n    for i in range(batch_size):\n        n_spk = num_speakers[i].item() if torch.is_tensor(num_speakers) else num_speakers\n        output = outputs[i, :, :n_spk]\n        target = targets[i, :, :n_spk]\n        \n        # Быстрая проверка перестановок (2 варианта)\n        if n_spk == 1:\n            loss = F.binary_cross_entropy(output, target, reduction='sum')\n        else:\n            loss1 = F.binary_cross_entropy(output, target, reduction='sum')\n            loss2 = F.binary_cross_entropy(output.flip(-1), target, reduction='sum')\n            loss = min(loss1, loss2)\n        \n        total_loss += loss / seq_len\n    \n    return total_loss / batch_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport json\nimport os\nimport numpy as np\nfrom itertools import permutations\nfrom sklearn.metrics import confusion_matrix\nimport math\nimport itertools","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_der(preds, labels, num_speakers):\n     \n    preds = preds > 0.5   \n    labels = labels.bool()\n    \n    total_errors = 0\n    total_frames = 0\n    \n    for i in range(len(preds)):\n        n = int(num_speakers[i].item())\n        pred = preds[i, :, :n]\n        target = labels[i, :, :n]\n        \n         \n        best_error = float('inf')\n        for perm in permutations(range(n)):\n            permuted_pred = pred[:, perm]\n            \n            tn, fp, fn, tp = confusion_matrix(\n                target.cpu().numpy().flatten(),\n                permuted_pred.cpu().numpy().flatten(),\n                labels=[0, 1]\n            ).ravel()\n            \n            current_error = fp + fn   \n            if current_error < best_error:\n                best_error = current_error\n        \n        total_errors += best_error\n        total_frames += target.numel()\n    \n    return total_errors / total_frames if total_frames > 0 else 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T23:54:36.274846Z","iopub.execute_input":"2025-06-16T23:54:36.275278Z","iopub.status.idle":"2025-06-16T23:54:36.280954Z","shell.execute_reply.started":"2025-06-16T23:54:36.275257Z","shell.execute_reply":"2025-06-16T23:54:36.280220Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def train():   \n    \n    model = EENDEDA(\n        input_dim=80,\n        hidden_dim=384,\n        num_layers=4,\n        n_speakers=21,\n        dropout=0.3\n    ).to(device)\n\n    \n    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-5)\n    criterion = AdaptiveFocalLoss(n_speakers=21).to(device)\n    metrics = DiarizationMetrics(max_speakers=21)\n    \n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'DER': [],\n        'precision': [],\n        'recall': [],\n        'f1': [],\n        'lr': []\n    }\n    \n    best_der = float('inf')\n    early_stop_patience = 5\n    early_stop_counter = 0\n\n    \n    for epoch in range(100):\n        model.train()\n        metrics.reset()\n        total_loss = 0\n        \n         \n        curr_length = min(15 + epoch//5, 30)\n        dataset.chunk_size = curr_length\n        \n        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\", total=len(dataloader)):\n            features, labels = batch\n            features = features.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            outputs = model(features)          \n            \n            spk_presence = labels.sum((0,1)) > 0\n            active_outputs = outputs[..., spk_presence]\n            active_targets = labels[..., spk_presence]\n            \n            loss = criterion(active_outputs, active_targets)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            metrics.update(outputs.sigmoid().detach().cpu().numpy(), \n                         labels.cpu().numpy())\n\n         \n        epoch_metrics = metrics.get_metrics()\n        epoch_loss = total_loss / len(dataloader)\n        \n        \n        history['train_loss'].append(epoch_loss)\n        history['DER'].append(epoch_metrics['DER'])\n        history['precision'].append(epoch_metrics['Precision'])\n        history['recall'].append(epoch_metrics['Recall'])\n        history['f1'].append(epoch_metrics['F1'])\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n        \n        \n        print(f\"\\nEpoch {epoch+1}:\")\n        print(f\"Loss: {epoch_loss:.4f} | DER: {epoch_metrics['DER']:.4f}\")\n        print(f\"Precision: {epoch_metrics['Precision']:.4f} | Recall: {epoch_metrics['Recall']:.4f}\")\n        print(f\"F1: {epoch_metrics['F1']:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n         \n        if epoch_metrics['DER'] < best_der:\n            best_der = epoch_metrics['DER']\n            early_stop_counter = 0\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'metrics': epoch_metrics,\n                'history': history\n            }, \"best_model.pth\")\n        else:\n            early_stop_counter += 1\n            if early_stop_counter >= early_stop_patience:\n                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n                break\n\n     \n    plot_training_history(history)\n    \n    return history, model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" def plot_training_history(history):\n    \"\"\"Визуализация метрик обучения\"\"\"\n    plt.figure(figsize=(15, 5))\n    \n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.title('Training Loss')\n    plt.xlabel('Epoch')\n    plt.grid(True)\n    \n    # Метрики\n    plt.subplot(1, 2, 2)\n    plt.plot(history['DER'], label='DER')\n    plt.plot(history['f1'], label='F1')\n    plt.title('Performance Metrics')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" if __name__ == \"__main__\":\n    res = train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}