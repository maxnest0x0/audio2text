{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54fe3e4d-9218-4f5e-ab6d-5fde4710ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a254e48-0c05-4814-a710-abd807867ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    D_MODEL = 256\n",
    "    N_HEADS = 4\n",
    "    N_LAYERS = 4\n",
    "    DIM_FEEDFORWARD = 1024\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LEN = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83819dc1-fa70-4661-bf2a-a028985cba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    SAMPLE_RATE = 16000\n",
    "    N_MELS = 80\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 256\n",
    "\n",
    "class AudioProcessor:\n",
    "    @staticmethod\n",
    "    def load_audio(path: str) -> torch.Tensor:\n",
    "        y, _ = librosa.load(path, sr=AudioConfig.SAMPLE_RATE)\n",
    "        return torch.FloatTensor(y).unsqueeze(0)  # (1, T)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_mel(waveform: torch.Tensor) -> torch.Tensor:\n",
    "        y = waveform.squeeze(0).numpy()\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=y,\n",
    "            sr=AudioConfig.SAMPLE_RATE,\n",
    "            n_mels=AudioConfig.N_MELS,\n",
    "            n_fft=AudioConfig.N_FFT,\n",
    "            hop_length=AudioConfig.HOP_LENGTH\n",
    "        )\n",
    "        return torch.FloatTensor(np.log(mel + 1e-8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "469bbe1f-a78b-43b1-b9be-e7da24edf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int = ModelConfig.D_MODEL, max_len: int = ModelConfig.MAX_LEN):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b6add36-341a-43ca-9a22-56db2823f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EENDModel(nn.Module):\n",
    "    def __init__(self, n_speakers: int = 2):\n",
    "        super().__init__()\n",
    "        self.n_speakers = n_speakers        \n",
    "        \n",
    "        self.mel_proj = nn.Linear(AudioConfig.N_MELS, ModelConfig.D_MODEL)        \n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding()        \n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=ModelConfig.D_MODEL,\n",
    "            nhead=ModelConfig.N_HEADS,\n",
    "            dim_feedforward=ModelConfig.DIM_FEEDFORWARD,\n",
    "            dropout=ModelConfig.DROPOUT\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer, ModelConfig.N_LAYERS)   \n",
    "         \n",
    "        self.head = nn.Linear(ModelConfig.D_MODEL, n_speakers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:        \n",
    "        x = self.mel_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2) \n",
    "        return torch.sigmoid(self.head(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b62c8d31-cbf7-4d96-a03c-a46ec086d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PITLoss:\n",
    "    @staticmethod\n",
    "    def compute(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        n_speakers = pred.shape[-1]\n",
    "        permutations = list(itertools.permutations(range(n_speakers)))\n",
    "        losses = []\n",
    "        \n",
    "        for perm in permutations:\n",
    "            permuted_pred = pred[:, :, list(perm)]\n",
    "            loss = nn.functional.binary_cross_entropy(\n",
    "                permuted_pred, target, reduction='none'\n",
    "            ).mean(dim=(1, 2))\n",
    "            losses.append(loss)\n",
    "        \n",
    "        losses = torch.stack(losses, dim=1)  \n",
    "        return losses.min(dim=1)[0].mean()\n",
    "\n",
    "class DiarizationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_paths: List[str], labels: List[torch.Tensor]):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        waveform = AudioProcessor.load_audio(self.audio_paths[idx])\n",
    "        mel = AudioProcessor.extract_mel_spectrogram(waveform)\n",
    "        mel = mel.squeeze(0).T   \n",
    "        label = self.labels[idx]  \n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128a99c-52fe-4bab-8864-b48a75738a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c762f28-d50d-43c6-b729-38900ee2981f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
